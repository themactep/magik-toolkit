###Generated by the compiler.  DO NOT EDIT!
###ChangeLog:

version-1.0.2_0.1.2:
0.NNA2:
    add parameter for generateBox(NNA1/NNA2);
    add serializer parameter check for reshape_layer_parser.cc(NNA1/NNA2);
    add gray->gray common_resize(crop) and test and fix bug in aip(NNA2);
    first layer ic1,oc=16 and conv_ic16_base.cc op support multi_batch; update its layer tests(NNA1/NNA2);
    first layer ic=1 oc=16/32 op support k5s2(NNA1/NNA2);
    fix bug in check LayerResource(NNA1/NNA2);
    check LayerResource in serial, fix bug in reshape_layer_parser(NNA1/NNA2);
    manager ORAM in data_stream_mgr/memory/memory_pool(NNA1/NNA2);
    update prelu/lrelu quant logic in licn bit4/pw bit4/base/fc ops(NNA1);
    updata drivers cal nmem size(NNA2);
1.NNA2:
    fix BUG reshape(NNA1/NNA2);
    update path of common files(NNA1/NNA2);
    fix bug in dma tests(NNA1/NNA2);
    support NV12 in Tensorx::step(NNA1/NNA2);
    deleta dma headfile in aie_kernel(NNA1/NNA2);
0.NNA1_2.NNA2:
    fix bug in serial(NNA1/NNA2); fix bug on softmax_nhwc_test(NNA1); fix bug in conv base bit8(NNA2);
    fix prelu logic in add_ubit8.cc(NNA1); fix bug about group in conv2d_bit8_kernel(NNA1);
    fix bug in aip(crop_common_resize,crop_affine)(NNA2);
    add oram strategy in licn bit8 s1/s2(NNA2); add aip_test in aip(NNA2); add bit4 slice function in concat_ubit8.cc(NNA1);
    remove first layer ic=1, oc=16/32 op limit in input width(NNA1/NNA2);
    fitst layer 4bit ops support prelu_lrelu act mode 3b2t/3b2s; update its debug src and layer test(NNA1);
    optimize softmax_nhwc when axis is 3(NNA1);
1.NNA1_3.NNA2:
    optimization hardswish logic in activation_layer(NNA1); add aip_test(NNA2);
    add dequantize_16bit_float in dequantize_layer(NNA1);
    fix bug in aip(writing out of bounds)(NNA2);
    fix bug in global_pool_src(NNA1/NNA2); fix bug in eltwise layer test(NNA1/NNA2);
2.NNA1_4.NNA2:
    fix bug in first layer op first_i8_ic1_l2.cc; code-format(NNA1/NNA2);
    add split box func in aip_affine(NNA2);
    add align fun in tensorx(NNA1/NNA2);
    simd op norm_ndhwc32 and add_4bit_scale_dma support multi_batch(NNA1);
    add test time in aip(NNA1/NNA2);
    licn_ifp1_k3s2 support stride=1; support oram bypass in resnet18(NNA2);
3.NNA1_5.NNA2:
    fix bug in aip(NNA2); fix bug in cache address alloc(first_i8_ic1_l2.cc NNA1/NNA2);
    fix bug in dump_utils(NNA1/NNA); fix bug in model_parser;(NNA1/NNA2);
    fix bug in custom internal lib(NNA1/NNA2); fix bug in dma_test(malloc oram_size > 384Kb NNA2);
    add new quantization method in mul layer(NNA1/NNA2); add licn ifull bit8 s2 v1(NNA1);
    add k55 bit8 s1 v1 to optimize ns+ss(NNA1);
    optimize affine_split in aip(NNA2); optimize licn and pw bit4 ops(NNA1);
    optimize licn and pw bit8 ops(NNA1); optimize licn ifp1(NNA1);
    optimize softmax_nhwc when axis is 3 and IC < 8(NNA1);
    conv_base_bit4 Operator function extension(input 4bit ==> output 8bit NNA1)

version-1.0.1_0.1.1:
0.NNA1:
    add perspective_nv12_nv12 and getPerspectiveTransform in perspective;add merge op(pw+pw+add);
    add oram strategy to licn_ifp1.cc; add sum_ndhwc32(axis:0 1 2 3); add FMU2\FMU4\FMU8\FMU8_H test in aip;
    optimize licn temp bit8 s1/2; update gru debug src;
    optimize speed of load_model;
    fix bug in dw base nna; fix bug in __aie_flushcache_dir/custom_opts.txt;
    first layer fix oram leak bug; first layer support pad after bn(preprocess); first layer support ic=1, oc=16/32 k5s4, k3s2;
    update its dbeug src and layer test;
    update base and pw op;
    move flushcache from objdeserial to model_parse;
0.NNA2:
    add parameter check in conv_base_bit4/conv_base_bit8/pw_v2_i48of
1.NNA1:
    add mutex in layer_parser; fix bug in multi-thread; add pad param check in pw ops
1.NNA2:
    add where ops all code;
    update conv base bit8; update licn ops;
    fix bug in op first_i8_ic1_l2 dma_out; fix bug in conv base bit8; fix bug in conv_ic16_bit8;
    remove __aie_set_nndma_unlink from fmt_convert/merge/first_layer/dw op;
    replace ddr_memalign/malloc by device_memalign/malloc for aip param;
    split dma code by NNA version;
2,NNA1_2.NNA2:
    fix bug upsample(NNA1/NNA2); fix bug in conv base bit8(NNA2);  fix bug in pixelshuff print info(NNA1/NNA2);
    remove gshufvb in nna2 ops(NNA2);
    add pad param check in pw and n1n ops(NNA1/NNA2); add t41 model test(NNA2);  add get_memory_size in tensorx(NNA1/NNA2);
    update lut in nna1/2 ops(NNA1/NNA2); update CMakeList.txt for toolchain(uclibc)(NNA1/NNA2); update gru and lstm test(NNA1/NNA2);
3.NNA2:
    fix logic error in conv_bit8_kernel; fix bug in upsample BANK_ROUND logic; fix bug in conv2d_bgra_kernel
    fix bug in aip(RMEM_PHYSICAL); fix BUG in where_base_its.cc;
    add new nna op licn_ifp1_k3s2.cc to support T41 yolov7 640*640; add where to types_utils.cc;
    add constructor in Tensorx; add in/out attr param check in model_parser; add in/out attr param check in base_layer_parser
    support dw base bit8 prelu bt quan; support first layer ic=1, oc=16/32 ops; support softmax_nhwc add W direction multi-channel;
    check returnvalue in dump;
4.NNA2:
    add nna op: conv_ic16_bit8.cc to support ic=16, oc=16;
    add new constructed function, add set_data in TensorM;
    add parameter check in add_layer_parser.cc, and upsample_layer_parser.cc;
    add DEBUG information for normalize_layer.cc;
    update venus_init/deinit; updata aip api name;
    fix bug in aip param check;
5.NNA2:
    add dma_manager; add set_data in Tensor; add aip_f Y_model;
    delete unused code; disable init_call_cnt in venus_deinit
6.NNA2:
    support batch in fc noalign base/fc_align/pw base/dw base nna; support aip_p RMEM func;
    fix bug in dw base nna;
    delete attach_kernel in children Layer;
7.NNA2:
    add crop_common_resize func in aip;  first layer ic=1, oc=16/32 op support BS act;
8.NNA2:
    add __aie_get_oram_info; add merge op conv3_fuse_add_bit8.cc to optimise Megvii multi_det;
    replace __aie_get_oram_size by get_oram_size; optimize licn and pw ops; update merge kernel;
    fix bug in aip flushcache RMEM;

version-1.0.0_0.1.0:
0.NNA2:
optimize gru nna op; optimize permute ndhwc32; updata aip_test; update kernel base_i8of;
    add slice_uint16/fp_32 slice_b4816_test; add new merge op(pw_dw_pw); add concat resource;add get step api;
1.NNA2:
    optimise nna2 op conv_base_bit8 logic; support format_convert_fp32 inplace; support do inference by step;
    add concat_float32_ubit8 in concat;  add k33+k33+add merge op; add add_ubit4 channel model;
0.NNA1_2.NNA2:
    optimize licn temp ifull bit8 s1;  update base_layer_parser; updata lstm;
    fix bug in format_covert_fp32/fc_align/fc_noalign;
    support ALL_SEPARABLE_MEM and SMART_REUSE_MEM mem_model in public; support add ubit8 channel model;
3.NNA2:
    add nna op conv_ic16_bit8.cc, support ic16_oc16(NNA2); add reduce_fp32_nhwc(NNA1);
    update debug_src and layer_test; update pw v2 bit8(NNA2); update nna2 conv and merge ops(NNA2);
    fix bug in wram judge(NNA2);fix bug about shift_count in conv_base_bit4(NNA1);fix bug in dw bit4(NNA2);
    check pointer before flush cache(NNA1/NNA2); remove input_w limit in first layer k3s2(NNA2);
4.NNA2:
    updata tool-chain(mips-linux-gnu-ingenic-gcc7.2.0-glibc2.29-Go-fp64-r5.1.5); update licn ops;
    optimize licn temp ifull bit8 s1/2;
1.NNA1_5.NNA2:
    fix bug in resize(load pixel)(NNA1/NNA2); fix bug in conv_base_kernel.cc(NNA2);
    fix bug in licn_ifp1 and support lut(output_float)(NNA2/NNA1); fix bug in layer test(NNA1/NNA2);
    remove output_chn in conv layer logic;
    add merge op(k33+pw)(NNA2); add shell for update version.log(NNA1/NNA2);
    add output_uint16 and output_fp32 in concat_layer(NNA1);
6.NNA2:
    conv base bit8 support prelu bt simd quan when output_bit4(NNA1);
    fix bug in conv base bit8(NNA2); fix bug in conv layer logic(NNA1/NNA2); fix bug in licn ops lut quant(NNA2);
    delete unused func in kernel(NNA1/NNA2);
7.NNA2:
    fix bug in layer test; fix bug in conv_ic16_base.cc;
    delete repeat/unused func in kernel;
2.NNA1:
    fix bug in landmark model test; first layer ops update DMA_IN logic(NNA1/NNA2);
3.NNA1:
    fix bug of custom lib(NNA1/2), add the getting operator names to DEBUG lib; replace VENUS_DEBUG by VENUS_INTERNAL_DEBUG;
    add pow_single_nhwc(NNA1/2);optimize layer::init(NNA1/2);fix bug in lstm debug src;
4.NNA1_8.NNA2:
    fix bug in multi_threads lock and unlock(NNA1/NNA2);
    optimise separableconv ops logic in s1 bank conflict(NNA2);separableconv op support oram ibuf_full(NNA2);

version-0.9.9_0.0.9:
0.NNA1_0.NNA2:
    fix bug in lut(of = fp32); fix bug in concat out_shape; add inplace logic;
    add mul_spatial_nhwc ops; add licn_ifp1_k1s1 op to optimise mobilenet_v2;
    update DRIVERS(NNA2); update pw base(NNA2); update conv2d_base_kernel(NNA2);
    add licn ifull bit8 s2(NNA2); add pw_v2(NNA2);
1.NNA1:
    fix bug in licn_temp_bit4_s1 and bit8_k123_nna;
    support generatebox block v1; add merge op;
2.NNA1_1.NNA2:
    fix bug in model_parse(NNA1/2);
2.NNA2:
    fix bug in aip(memcpy ptr error);
3.NNA2:
    fix bug in aie_mmap, disable dma_unlink in NNA2; fix bug in pw base;
    add merge layer(pw + pool + pool + pool + concat);
4.NNA2:
    fix bug in aip(_flushcache_);
    add ctrl_flag check in model serial; add resize param check;
    add merge layer(pw+pool5x5+pool9x9+pool13x13);
3.NNA1:
    update gru; support lut in dw licn and pw(NNA1/NNA2);
    add embedding_nhwc;separableconv support simple relu activation;add pw_dw_bit8s1_v1 nna op to optimise mobilenet_v2;
    add aip param judgement(NNA2);
5.NNA2:
    support lut in fc_align_base ,fc_noalign_base;
    optimise relu act int32_to_int8
6.NNA2:
    conv_base_bit8 support output fp32 when lut act;
    add gru; add crop aip; add aip test case; fix bug in add lut_001;
4.NNA1_7.NNA2:
    support F10 in gru nna op(NNA1/NNA2); optimize dw base nna;
    update lut_002(NNA2);  update fc_align/noalign_base lut(NNA1/NNA2);
    add sqrt_nhwc;  add dequantize_nhwc_ubit8 in dequantize layer; add aip_tests;add constant layer; add merge op pw_pw;
5.NNA1_8.NNA2:
    support first layer ibitw=12(NNA2);
    updata first layer k3s2 block logic(NNA2);
    add gray model in crop_resize(NNA2/NNA1);
6.NNA1_9.NNA2:
    update licn bit8 s1/2 and fix bug in pw(NNA2); optimize quant int32_to_int8 in licn ops(NNA2);
    fix bug in cva(affine gray->gray/base1==NULL/Return in advance)(NNA1/NNA2);

version-0.9.8_0.0.8:
0.NNA1_0.NNA2:
    update DRIVERS(NNA1/2);
    add output clip func; Custom add definitions to kernel.cc;
    fix bug in activation;fix bug in add kernel; fix bug used get_optname_from_func;
    updata add_ubit4 logic; add mul_ubit8_nhwc ops(signal,channel) and eltwise for nhwc(16bit,32bit),ndhwc32(32bit);
    add merge ops(conv_k33+pw+pw;add+pw)(NNA2);
    fix bug in licn_ifull_bit8_s1 block logic(NNA2); fix bug in add_base_nna(NNA2);
1.NNA1_1.NNA2:
    fix bug in mul; fix bug in add; fix bug in merge;
    add upsample; optimize conv2d(for yolov3);
    fix bug in k77 bit8 s2(NNA2); optimize licn bit8 s2(NNA2); update licn ops(NNA2);
2.NNA2
    fix bug lut(conv_base_bit8 ops); fix bug in merge kernel;
    first layer k55 support stride=4; add equal ops;
    optimize licn bit8 s1;

version-0.9.7_0.0.7:
0.NNA1_0.NNA2:
    update gru(quant by simd); add pool_base_ubit4;
    support normal block in bgra/nv12 conv2d ops; support simple block in conv2d ops;
    update get_cpu_frequency for t41(NNA2);update get_wafer_status; fix bug in fc_bit8;
1.NNA1:
    add sqrt/dequantize/cast; support ShareMemoryMode::SMART_REUSE_MEM for internal;
    optimize bgra/nv12 conv2d ops; update reshape;
2.NNA1:
    add reduce(sum,mean)/pow; fix bug in pool/dequantize; update slice/cast; add set_input_channel_layout;
1.NNA2:
    fix bug in set_input_channel_layout;
3.NNA1:
    support dequantize in pool/mul;optimize preprocess in first bgra layer;
    support ReturnValue/block in first bgra layer(NNA2);
    support block in conv layer(NNA2); fix bug in NNA1 ops ReturnValue;
    fix bug of color order in first layer;
    fix bug in mul (inputs not match in_offset/scale);
    update ObjectBbox_t/Box_t, delete ObjBox/ObjBbox;
2.NNA2:
    optimize act_relu bs_quan logic in first layer(NNA2);
    update get_forward_memory_size;
4.NNA1:
    optimize nna2 licn ops(NNA2); add generatebox layer; add get_objbox_from_output in postproc; fix bug in aie_lock/unlock and aip;
3.NNA2:
    support block in normalize_nhwc, upsample, pool; add depthwise base bit4(NNA2);
	fix bug in generatebox; add merge layer(NNA2); optimize licn ops(NNA2);
	check DRIVER_VERSION(NNA2); update first layer kernel; add add_ubit8_nhwc;
    the custom version generates the required functions and definitions;
5.NNA1_4.NNA2:
    add licn ifull bit8 s1 op(NNA2); add upsample fp32 nhwc;
    fix bug in upsample; optimize pw op(NNA2);
    fix bug in last layer shift count; add equal ndhwc32, update equal nhwc;
    fix bug in venus init;

version-0.9.6_0.0.6:
0.NNA1:
    updata DRIVERS for multi-process;
    add mat_mul in postproc; fix bug in licn ops;
    support concat in height;
0.NNA2:
    add if8off in avgpool; update gops/nna_usage for internal;
    update group-conv2d;update mul ops; add separableconv op(NNA2);
    add profiler of separable layer;
1.NNA1:
    fix bug in fmt(FP12); support reshape(NDHWC32);update block;
    fic bug in pool; update concat layer serial/parser;
    update activation (support arbitrary input/output data type; support hardswish);
    support negtive param(start/end) in slice layer;
    support concat in axis=0;
2.NNA1:
    support h-block in add ops; extend padding ops(F4/8/16/FP32);
    support slice in N/C; support FP32 in mul op; support FP32 output in concat;
    update h-block logic in conv2d ops; support ShareMemoryMode::ALL_SEPARABLE_MEM for internal;

version-0.9.5_0.0.5:
0.NNA1:
    fix bug in LayerParam/LayerRes(shared_ptr); fix bug in licn ops;
    fix bug and add free_data in Tensor/TensorM; add OHWI in types;
    support matmul layer parser; fix bug in FP16 convolution;
    fix bug in DRIVERS(memory may be overlap); fix bug in separable layer;
1.NNA1_0.NNA2:
    support conv2d_fp32_ohwi; fix bug in group base;

version-0.9.4_0.0.4:
0.NNA1_0.NNA2:
   update BaseNode; support merge add+normalize; support fixed add op in NNA1;
   replace dtype/dformat by AUTO in kernel_type; add param in softmax;
   update attention op;
1.NNA1:(version 1 is lost)
   support resize FP16; fix bug in pool; support start/end in softmax;
2.NNA1:
   fix bug in version.cmake
   optimize generate_box; support crop_common_resize;
   support data_type in Tensor/TensorM;
   support Prelu/lrelu in first/fc(NNA2) ops;
   fix bug in dw with dilation;fix bug in debug_src;
   fix bug in DRIVERS(conflict func name);
3.NNA1:
   support FP16 in embedding; support FP32 in resize;
   fix quantize_type bug in dw; fix ctrl_flag bug in extern ops;
   use out_bias as float value in add_nna;
1.NNA2:
   support separableconv layer;define AIE_VERSION IGNORE for parser test;
4.NNA1_2.NNA2:
   fix bug in k55_bit4_s1;
   update prelu/lrelu in bit4 ops; update dump to support multi-outputs; update fmt_convert op;
   remove prelu/lrelu_sign; support prelu/lrelu by bs in bit4 ops;
   fix mudata bug in kernel when _op_ptr is used(bugs occurs when free_inputs/forward_memory is called);
   fix bug in free_inputs/forward_memory(bugs occurs when SHARE_ONE_THREAD mode is used);
   fix bug of shared_ptr in MBuffer(unpredictable error maybe occurs, pelease update);
   fix bug of parser/serial in separable_convolution;
   fix bug in maxpool(bit8);

version-0.9.3:
1. add enable_cnt_input_pad, out_scale for post_quantize; update gru op;
   support F16 NHWC_TO_NDHWC32; support reshape F8 NDHWC32;
   support get summary per frame;
2. update DRIVERS(memory leak); fix bug in get_output_by_name; update profiler;
   update interface; fix bug in perspective(3chn); update pool; support concat(FP4);
3. fix bug in venus.m; update QuantizationType
4. add check_layer_paramter(shift_count); support mixed bitwidth in add;
   update enum types; fix bug in base kernel;
5. add c api for venus inference; update attention; support FP12 output in normalize;
6. fix bug in attention_m0;

version-0.9.2:
1. update normalize;support moments(nhwc fp32);support attention(nhwc);support lut in convolution/add bit8
   update DRIVERS(fix alloc failed in ocr agt);support F10 in gru op;
   add tsm layer;
   (bugs in serial/parser, please update)
2. fix bug in serial/parser;
   update attention layer;
   compress code size in first layer to fix bug when compiling;
   support output FP32 in upsample(FP Resize);
   support F8 in gru op;
   support LUT in first layer and FC(bit8) ops;
   fix bug(pads) in pool ops;
   fix bug in normalize op;
3. fix bug in upsample(pixelshuffle);
   fix bug in first layer(gray, DMA bug);
   support NHWC4_TO_NDHWC32(F16);
   update lstm(bit8);
   optimize lut(bit8);
4. update permute/slice;
   support k55/stride4/OF8 in first layer;
   fix bug in upsample;
   update normalize;
   support activation;
5. add spacebatch;
   fix bug in get_input(index);
   fix bug in cache flush;
   support post_quantize in add;
   fix bug in Tensorx::get_bytes_size;
6. support pad_value in upsample;
   add api to free inputs/outputs memory in network;
7. update first ops;
   add api to free inputs/outputs memory in netbase;
8. fix bug in attention;
   support pw in base444 kernel;

version-0.9.1:
1. support slice in concat;support FP32 in normalize;support dilation in dw(bit8)
2. fix bug in model parser
3. fix bug in concat(slice)
4. update gru;
5. replace gru output fmt by NDHWC32
6. fix bug in group(bit8)
7. support ksize<=3 in k77 op; support bt in conv(bit8 ic32)
8. support new prelu/relu method in conv2d(bit8); fix oram memory leak problem of concat;
9. support prelu/sigmoid/tanh/silu/swish in fc(bit8) op;add i8of in add op;support bt quantize method in base op;
10. fix bug in mul ops;

version-0.9.0:
1.update usage of scale in add op
2.update nmem monitor in DRIVERS
3.fix bug in dump
4.support start_off in load_model;update gru op
5.fix bug in licn op; update resize
6.support gcc720 Release5.1.0 glibc2.29 compiler
7.fix bug in model_serial CMakeList;optmize resize
8.support special param(stride2x1) in bit8 op; update add op;
9.fix bug in mul(nhwc);
10.fix bug in eltwise(NHWC); add imgproc sample

version-0.8.9:
1. update scale in first op; add out_bias in add op;support w10 in first/dw ops; fix bug in bit8_k123_nna
2. fix bug in data dump; disable MLOGE in release version
3. fix bug in add op;

version-0.8.8:
1. support scale in upsample(NEARST/BILINEAR) op; support embedding op; fix bug in pw; fix bug in resize(bgra w/h not alin2);
   LIGHT_RELEASE_ENABLE(disable bit2 ops);support k66s2 in first layer;
2. support k66s1 in first layer; fix bug in serial lib compile
3. updata bscaler; fix bug in upsample; enable pw_if2offw4 op;
4. fix bug in bs
5. fix bug in bs header files
6. test
7. fix bug in release flow
8. support batch in base/dw/k55/n1n/pw(bit4);support get_affine_transform; fix bug in bit8; replace kernels by scales in upsample;
9. support batch in first layer(bit8); optimize base op(bit8); add i8o4w8; support old/new soc nna driver;fix bug in tensor; fix bug in upsample(bit2/4)
10. support batch in first layer(bit4); optimize k77s2 bit8 op;
11. update DRIVERS
12. commile venus.m for nmem msg output; fix bug in pool parser
13. fix bug in driver and support venus.m; fix bug in conv base op
14. fix bug in bs; add F12 dw/eltwise/concat/pool/normalize/mul op; update embedding;add alpha in add op; replace int data type by float in nms
15. fix bug in nms; add pws1488/844,k33s2844 ops
16. fix bug in pwv2(bit8); fix bug in mul(in_offset); add F12 upsample; updata offset in first ops; support ARGB in BS
17. fix bug in avgpool; add F12 in first op;
18. support RGB888 in imgproc; fix bug in F12 kernel
19. fix bug in pw844; update mul(spatial)bit2/4 op; add F10 k33s2/eltwise_max/eltwise_min ops
20. fix bug in pad(pool)
21. fix bug in normalize(width noalign);fix bug in multi_num serial(add);support batch in concat
22. fix bug in batch add/mul(bit8);support base_i8offw4; support slice in batch dim; fix dma bug in k1n/n1 ops;
23. update base kernel; first release venus for t40n;
24. support 4bit linear method in upsample op; modify sign bit of offset in mul ops;support w10 in F10 base/licn ops;support batch in fmt op;add out_bias in add ops; release first evaluate tools

version-0.8.7:
1. support input_chnlayout, fix bug in model serial, modify g_version_magic_number public
2. add slice layer prase/serial
3. fix bug of install header files
4. add mutex in ddr_mem; update slice param
5. add param in eltwise for post quantize; fix compiler bug in ddr_mem
6. fix bug in slice param serial
7. support swish/sigmoid/silu/tanh in dw(bit8) op;support post quantize in eltwise
8. fix bug in eltwise param serial/parser;add softmax layer;
   support swish/sigmoid/silu/tanh/prelu/lrelu in bit4 ops; add venus_get_used_mem_size for internal
9. optimize add(tanh) op; support softmax;support padding;fix bug in mul ops
10. support nhwc_to_ndhwc32,nhwc_to_nhwc4; support batch in base/add/pool/mul/fc(bit8) op; support Td(time temporal different) layer
11. fix bug in bs(resize);optimize similar_transform; support padding in c; support permute layer

version-0.8.6:
1. replace swish beta 1.5f by 1.0f
2. infer output shape in slice
3. add output_hw support fc in H or W dimension
4. add fc+slice in layer_runtime_info; add calc_op func; fix bug in fc+slice parser/serial;
5. add input_chnlayout; fix bug in slice output infer

version-0.8.5:
1. support spacetodepth(focus and passthrough);
2. add api to get version of venus; bug of ceil_mode fixed;
3. add optimized pw(bit8) op (yolov3_416_bit8_nna_usage=63%);
4. update layer param of upsample to support configure the location of real value;
5. add layer interface and model parser of normalize layer, support normalize layer;
6. supporting gray input in first layer;
7. support coord-convolution and deconvolution;
8. upgrade DUMP_CONV2D_WBT to DUMP_LAYER_RESOURCE;
9. add uint8 support one input add const;
10. supporting dilation convolution(8bit);
11. support 8bit and 4bit lstm operator;
12. bit8_fc support larger input channel(>8192);
13. add optimized 8/4bit convolution operator;
14. add optimized bilinear resize(scale = 2) upsample operator;
15. some other critical bugs have been fixed;
16. add out_scale for post quantize in add; fix bug in first layer(focus222); support swish in first and base op;
17. fix bug in first op; add slice base api;
18. add param for multi-inputs in add;
19. add param in conv param for fc+slice;add eltwise layer;add debug code;

version-0.8.4:
1. adding output saturate to bit2/4/8 convolution ops;
2. supporting relu activation function in convolution of float output;
3. add bilinear type and aligned_corners to UpSample layer param and supporting 8bit bilinear upsample;
4. add optimized 2/4/8bit convolution ops;
5. add some check for memory from external;
6. support lstm ops;
7. support 8bit fc ops of 32 channels input;
8. model parser is compatibles with the old version(<=0.7.4) models;
9. some other critical bugs have been fixed;
10. remove ADK;

version-0.8.3:
1. warm up 5 times in profiler;
2. update generate_box to fix some bug;
3. add optmized k33 licn(bit4, s2, IC<=512) ops;
4. add lstm layer param, layer parser and infer_output_shape of lstm layer;
5. support aligned/non-aligned output in global-avgpool(bit2/4/8);

version-0.8.2:
1. add 1xN and Nx1 convolution operators(bit4/bit8);
2. support ksize>4 in maxpool(bit2/4);
3. bug of fc and nna3 fixed in bit8_k123_nna3 operator;
4. add 5x5 convolution(i8o8s1, IC<=64) and bit8 mul operator;
5. update mul layer param to support aligned and non-aligned bordcast tensor;
6. rename ddr_malloc/ddr_memalign/ddr_free to nmem_malloc/nmem_memalign/nmem_free in user's api file;
7. add optimized k33_licn(bit4, s1/2, IC<=512) and add operator;
8. update imgporc api(warp/crop_affine and warp/crop_perspective);

version-0.8.1:
1. update the imgporc interface and support the configuration of pad value;
2. add nna usage rate for profiler;
3. add ddr_malloc/ddr_memalign and ddr_free api to alloc memory from nmem;

version-0.8.0:
1. update profiler(add bandwidth);
2. the layer interface is rewritten in a modular way to facilitate the subsequent
   implementation of the combination operator interface;
3. update first layer(bgra format) to process situation of input width no align 16;
4. modify imgproc api to support padtype(bottom-right or fill around);
5. add focus ops in first layer(focus2x2, obit=2/4, stride=1);
6. add ceil_mode for pool param;
7. fix some bug of 8bit convolution and maxpool layer;
8. model parser is compatibles with the old version(<=0.7.3) models;
9. some other critical bugs have been fixed;

version-0.7.3:
1. add model type to distinguish internel model from public model;
2. support focus module in first layer, and add optmized pw(bit2) kernels;
3. add mutex to ensure that the inference process is not interrupted by thread;
4. add shift_count in layer param to shift the result of convolution;
5. model parser is compatibles with the old version(<=0.7.2) models;
6. some other critical bugs have been fixed;

version-0.7.2:
1. add example code to set cpu affinity;
2. update layer interface and model parser of mul layer;
3. add optimized add_ubit2, pw_v2, mul(2/4bit) kernels;
4. update dma wait method;
5. 8bit convolution layer support fc and group;
6. add focus module in convolution layer interface;
7. model parser is compatibles with the old version(<=0.7.1) models;
8. some other critical bugs have been fixed;

version-0.7.1:
1. add some check to catch internal error;
2. some bug fixed in 8bit convolution layer;
3. bug of dma wait and flushcache fixed;
4. generate_box support yolov5 postprocess;
5. add broadcast mul layer interface and model parser;
6. some other critical bugs have been fixed;

version-0.7.0:
1. update profiler info of depthwise convolution;
2. change depthwise weight from symm signed to signed;
3. add optimized pw_v1 kernels(bit2/4) and maxpool222 kernel;
4. support base convolution of kernel w/h <= 3;
5. support convolution 7x7 for bgra/nv12 input layer(2/4bit, first layer);
6. support ubit8 maxpool operator, bug fixed in add_ubit8,bit8_k123_nna2,bit8_k123_nna3 kernels;
7. add DUMP_OUTPUT_FEATURE DUMP_INPUT_FEATURE DUMP_CONV2D_WBT to dump some data;
8. add flush cache in device_malloc and device_memalign;
9. support dump feature data in NHWC format;
10. add dma wait before netwotk has been inited;
11. add bit8_depwise_nna,bit8_group_nna;
12. support uint8 output in base convolution kernels; support group convolution in k33ifp1/ifpge kernels;
13. update venus api to support internal(without adk) and public(with adk) inference, merge internal and public api in an venus library;

version-0.6.1:
1. remove unroll-all-loops compiler param to compress the size of veuns lib;
2. update CMakeLists to merge all archive;
3. support line_align for uint8 tensor;
4. add pixel_shuffle layer, shorcut support scale round quantization
5. add adk for user's api;
6. reserve (stride_h*64)KB memory to cache BGRA data and some temporary data;
7. support group convolution;
8. modify bscaler color cvt(ddr->ddr) in first layer;
9. modify aie_mmap maps nmem according to cmdline;

version-0.6.0:
1. bgra kernels support 2/4bit weight;
2. all 2bit and 4bit kernels support 2/4bit hybrid weight;
3. update profiler summary format;
4. 2/4bit fc kernels support 8bit weight;
5. add optional parameter line_align in tensorx;
6. support 8bit conv kernels;
7. support 8bit concat layer and kernels;
8. support 2/4bit depthwise conv;
9. add optimized 2/4bit add kernels;
10. update tensor api to support copy constructed func and ref_count;
11. add imgproc api based on bscaler;
12. update DataFormat to support NV12, and update network api to support NV12 input data;
13. add generate_box and memcopy api for postproc;
14. update rmem to nmem, add some demo;
15. fixes some critical bugs;

version-0.5.1:
1. support fc layer and kernel;
2. suooprt profiler;
3. add optimized conv1x1 kernels;
4. add nms api for network post process;
5. fixes some critical bugs;

version-0.5.0:
1. support load memory model;
2. support encrypt/decrypt model file;
3. support pad value in first layer kernels;
4. add pool layer interface and maxpool kernel;
5. add facerec demo;
6. update data format
7. support large input channels for conv2d;
8. support ib2ob4 and ib4ob2 in conv2d kernels;
9. add base kernels for 3x3 convolution, input bit2/4, output bit2/4/32/float;
10. all kernel support float and int32 output;
11. fixes some critical bugs;
12. support data format convert layer, eg: NDHWC32->NHWC;

version-0.4.1:
1. support preprocess, pad value, post quan in first layer;
2. add some debug info for bgra kernel;
3. support upsample kernel and layer;

version-0.4.0:
1. reomove nn_dma_driver;
2. tensor support align every line to 64bytes;
3. support dump in/out data to debug;
4. merge 2bit conv2 c code and add Add layer api
5. support multi scale input;
6. fix wrong filter size in first layer;
7. support 4bit convolution kernels
8. support valid pad in first layer

version-0.3.0:
1. hidden internal symbol to optimize the size of libvenus.so;
2. add conv2d_kernel(2bit), fix bug in init_nna_conv2d_param;
3. add some debug info, add yolov3 demo;
4. update mudata api, addrs of mbuffer align to 64 bytes;
5. support add layer(uint2b/4b) kernel ;
6. add pw kernel(uint2b/4b), and remove log2 in pw layer;

version-0.2.0:
1. add aie drivers and tools; add some files of nna;
2. update test case;
3. move drivers from src/kernels/aie to src/.;
4. add pool upsample add layer param and model parser;
5. merge some code from aie_kernel;
6. add conv2d_k3g1o0s2p1l4;
7. add aie.cmake to build kernel aie code;
8. add bgr kernel;
9. fixes some critical bugs;

version-0.1.1:
1. add clang-format;
2. std::atoi bug fixed;
3. add model for model_run demo;

version-0.1.0:
1. add user API;
2. add core module;
3. add memory pool;
4. add model serial and model parse;
5. add base type define;
6. add type system;
7. add layer and kernel demo;
8. add Convolution2D(float);
9. add model run demo;
10. add some utils module for internal;
11. add unit test module;
12. add aie drivers and some files of nna;
13. add tools to build aie code
14. add venus init and deinit api
